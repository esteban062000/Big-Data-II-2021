{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRMHl02Y7vYh"
      },
      "source": [
        "# Neural Network Models for Combined Classification and Regression\n",
        "\n",
        "Algunos problemas de predicción requieren tanto valores numéricos como categóricos para la misma entrada de datos. Una manera de resolver estos problemas es utilizar modelos predictivos de regresión y clasificación, sobre los mismos datos, y utilizar los modelos secuencialmente.\n",
        "\n",
        "Otra forma (y, a menudo, más efectiva), es desarrollar una sola red neuronal que pueda predecir tanto un valor numérico como un categórico para la misma entrada. Esto se conoce como un **multi-output model** y puede ser relativamente fácil de desarrollar con las librerías **Keras** y **TensorFlow**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmDJ_-IE9OV-"
      },
      "source": [
        "## 1. Single Model for Regression and Classification\n",
        "\n",
        "Un problema de tener dos modelos distintos (uno para regresión y otro para clasificación), es que las predicciones pueden diverger entre ellos. Con un solo modelo, se tiene la ventaja de que se puede actualizar y mantener más fácilmente, ofreciendo mayor consistencia en las predicciones de ambos tipos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9MFiQWw9Rob"
      },
      "source": [
        "## 2. Separate Regresion and Classification Models\n",
        "\n",
        "En esta sección, se elegirá un *dataset* real donde se necesiten predicciones de regresión y clasificación al mismo tiempo, y luego se desarrollarán modelos separados para cada tipo de predicción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU9jVhia9VP3"
      },
      "source": [
        "### 2.a Abalone Dataset\n",
        "\n",
        "Este *dataset* describe los detalles físicos de *abalone* (abulón, tipo de molusco) y requiere predecir el número de anillos que posee, el cual es un aproximado de la edad de la creatura. La \"edad\" puede ser predicha como un valor numérico (en años) o una clase categórica (el año ordinal como una clase)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWBzYtVJASMs",
        "outputId": "94e9f447-bb56-4812-b766-abd4e1360f43"
      },
      "source": [
        "# load and summarize the abalone dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "# summarize shape\n",
        "print(dataframe.shape)\n",
        "# summarize first few lines\n",
        "print(dataframe.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177, 9)\n",
            "   0      1      2      3       4       5       6      7   8\n",
            "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
            "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
            "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
            "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
            "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pfO-msAWU9"
      },
      "source": [
        "Se puede observar que existen 4177 ejemplos (filas) que se pueden utilizar para entrenar y evaluar el modelo y 9 *features* (columnas) incluyendo la variable de salida. Todas las variables de entrada, menos la primera, son numéricas. Para facilitar la preparación de datos, se eliminará la primera columna y se concentrará en modelar los valores numéricos de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whaOhUzW9YfB"
      },
      "source": [
        "### 2.b Regression Model\n",
        "\n",
        "Además de separar las columnas de entrada y salida, también se forzarán todas las columnas de entrada a que sean de tipo *float* (el esperado por las redes neuronales) y guardar la cantidad de *input features*, que será necesaria para construir el modelo más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUbaOaVBbM1"
      },
      "source": [
        "...\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elc7BLnvBdF9"
      },
      "source": [
        "Seguidamente, se puede dividir el *dataset* en el conjunto de entrenamiento y el conjunto de evaluación. Vamos a utilizar una muestra aleatoria de 67% del *dataset* para entrenar el modelo y el 33% restante para evaluarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT1kG98gBpVC"
      },
      "source": [
        "...\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCJbrO_Bqdw"
      },
      "source": [
        "Seguidamente, se puede definir el modelo de un *multi layer perceptron*. Se tendrán dos capas ocultas, la primera con 20 nodos y la segunda con 10 nodos, ambas utilizando la función de activación ReLU y una inicialización de pesos *\"he normal\"* (una buena práctica). El número de capas y nodos fue elegido arbitrariamente. \n",
        "\n",
        "La capa de salida tendrá un solo nodo para predecir un valor numérico y una función de activación linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6eQX_c5CHIP"
      },
      "source": [
        "...\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5gse1bCH56"
      },
      "source": [
        "El modelo será entrenado para minimizar el MSE *loss function* utilizando la versión efectiva **Adam** del gradiente del descenso estocástico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIPby5QRCS4v"
      },
      "source": [
        "...\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpzJroACT96"
      },
      "source": [
        "El modelo se entrenará durante 150 *epochs* y tendrá un *mini-batch size* de 32 muestras, nuevamente, parámetros elegidos arbitrariamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3utJih6DhiG"
      },
      "source": [
        "...\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfRNa35UDkXF"
      },
      "source": [
        "Finalmente, una vez el modelo ha sido entrenado, se evaluará y se reportará el MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMLmlzeXDp_J"
      },
      "source": [
        "...\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvohw00jDrsh"
      },
      "source": [
        "Al unir todas las partes, se tiene el siguiente código de una red neuronal MLP para el *dataset* tratado como un problema de regresión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAi3e1AJD2EW",
        "outputId": "3c721d9e-8778-4e98-9817-119fd28bb151"
      },
      "source": [
        "# regression mlp model for the abalone dataset\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 59.4564\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 12.1797\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 8.0399\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 7.5015\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 7.2487\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 7.0616\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 6.9186\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 6.8042\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 6.6513\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 6.4886\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 6.2426\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 6.0605\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 5.8669\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 5.7058\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 5.5454\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 5.3999\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 5.2980\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 5.1967\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 5.1850\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 5.1084\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 5.0649\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 5.0192\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 4.9980\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 4.9707\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 4.9920\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 4.9791\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 4.9400\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 4.9745\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 4.9323\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 4.9495\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 4.9594\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 4.9417\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 4.9469\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 4.9555\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 4.9177\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 4.9299\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 4.9124\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 4.9313\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 4.9054\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 4.9213\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 4.9093\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 4.9349\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 4.9191\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 4.9197\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 4.9406\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 4.9897\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 4.9356\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 4.8964\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 4.9502\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 4.9105\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 4.9079\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 4.8934\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 4.8943\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 4.9090\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 4.9021\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 4.8743\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 4.9405\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 4.8831\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 4.9338\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 4.9011\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 4.8899\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 4.8840\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 4.8707\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 4.8985\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 4.8934\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 4.8683\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 4.8959\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 4.9104\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 4.8718\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 4.8767\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 4.8926\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 4.9005\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 4.8706\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 4.8568\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 4.8858\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 4.8900\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 4.8521\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 4.8567\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 4.8484\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 4.8510\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 4.8711\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 4.8674\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 4.8227\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 4.8403\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 4.8293\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 4.8539\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 4.8477\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 4.8368\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 4.8488\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 4.8652\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 4.8611\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 4.8384\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 4.8227\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 4.8198\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 4.8100\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 4.8049\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 4.8272\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 4.8074\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 4.8213\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 4.8343\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 4.8199\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 4.8383\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 4.8141\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 4.8182\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 4.8183\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 4.7806\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 4.7890\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 4.8080\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 4.8177\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 4.7971\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 4.7987\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 4.7837\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 4.7878\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 4.7691\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 4.7950\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 4.7955\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 4.7837\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 4.7316\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 4.7778\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 4.7539\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 4.7419\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 4.7354\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 4.7313\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 4.7602\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 4.7573\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 4.7093\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 4.7270\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 4.7169\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 4.7326\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 4.6958\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 4.7425\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 4.7211\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 4.7019\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 4.6986\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 4.6973\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 4.6950\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 4.6920\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 4.7142\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 4.6946\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 4.6851\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 4.6798\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 4.6512\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 4.6886\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 4.6754\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 4.6612\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 4.6893\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 4.6684\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 4.6259\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 4.6485\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 4.6568\n",
            "MAE: 1.547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzQZLEQpD85i"
      },
      "source": [
        "Al correr el código se prepara el *dataset*, se entrena el modelo y se reporta un estimado de su error, en este caso, cercano a 1.5 (anillos).\n",
        "\n",
        "Ahora, veamos un modelo similar para clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-NhT7L9avb"
      },
      "source": [
        "### 2.c Classification Model\n",
        "\n",
        "El *abalone dataset* puede ser tomado como un problema de clasificación donde cada entero \"anillo\" es considerado como una clase categórica. El ejemplo es muy similar al anterior de regresión, con algunos pocos cambios.\n",
        "\n",
        "El primer cambio requiere asignar un entero separado para cada clase de \"anillo\", empezando en 0 y terminando en el número de clases menos 1. Esto se puede hacer con el **LabelEncoder**. También, se puede guardar el número total de clases, para utilizarla más adelante en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQQFiMYRGyNT"
      },
      "source": [
        "...\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtqZRQ6_GzYz"
      },
      "source": [
        "Luego de dividir los datos en los conjuntos de entrenamiento y evaluación, se puede definir el modelo y cambiar el número de *outputs* para que sea igual a la cantidad de clases y utilizar la función de activación *softmax*, común para problemas de clasificación multi clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ7ooyXaG_qx"
      },
      "source": [
        "...\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RlMjHP_HBIC"
      },
      "source": [
        "Ahora, se puede entrenar el modelo al minimizar la *sparse categorical cross-entropy function*, apropiada para problemas de clasificación multi clase con clases codificadas como enteros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KDeFs7HWgc"
      },
      "source": [
        "...\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TdXmHV4HXK6"
      },
      "source": [
        "Luego de que el modelo es entrenado, se puede evaluar su rendimiento al calcular la precisión en la clasificación con el conjunto de datos para evaluación, conocido como *hold-out set*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzi3aHVEHgzw"
      },
      "source": [
        "...\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKUkaargHkAo"
      },
      "source": [
        "Al unir todo esto, se tiene el siguietne código para un problema de clasificación sobre el *abalone dataset*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9B_ARg-HqFL",
        "outputId": "fb022ec0-6292-400e-dae3-e13cca24b8a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# classification mlp model for the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y))\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 3.3437\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 2.9312\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 2.5832\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 2.4424\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 2.3703\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 2.3202\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 2.2816\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 2.2515\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 2.2236\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 2.1992\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 2.1837\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 2.1729\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 2.1589\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 2.1518\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 2.1420\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 2.1341\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 2.1263\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 2.1213\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 2.1144\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 2.1071\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 2.1013\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 2.0954\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 2.0895\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 2.0828\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 2.0773\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 2.0721\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 2.0656\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 2.0595\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 2.0529\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 2.0483\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 2.0418\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 2.0375\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 2.0302\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 2.0271\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 2.0206\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 2.0155\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 2.0102\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 2.0070\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 2.0014\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 1.9974\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 1.9942\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 1.9919\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 1.9857\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 1.9847\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 1.9811\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 1.9774\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 1.9742\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 1.9722\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 1.9716\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 1.9663\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 1.9653\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 1.9656\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 1.9620\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 1.9603\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 1.9586\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 1.9572\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 1.9557\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 1.9567\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 1.9539\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 1.9533\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 1.9530\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 1.9494\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 1.9513\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 1.9482\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 1.9475\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 1.9448\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 1.9459\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 1.9469\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 1.9447\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 1.9442\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 1.9413\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 1.9427\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 1.9402\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 1.9414\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 1.9414\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 1.9421\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 1.9400\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 1.9397\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 1.9376\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 1.9388\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 1.9377\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 1.9368\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 1.9377\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 1.9360\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 1.9354\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 1.9350\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 1.9381\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 1.9369\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 1.9349\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 1.9352\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 1.9339\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 1.9334\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 1.9337\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 1.9324\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 1.9338\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 1.9320\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 1.9319\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 1.9319\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 1.9318\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 1.9321\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 1.9316\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 1.9323\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 1.9318\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 1.9312\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 1.9305\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 1.9288\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 1.9292\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 1.9307\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 1.9296\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 1.9296\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 1.9281\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 1.9290\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 1.9285\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 1.9268\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 1.9271\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 1.9278\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 1.9271\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 1.9277\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 1.9255\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 1.9282\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 1.9262\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 1.9251\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 1.9273\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 1.9268\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 1.9261\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 1.9277\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 1.9234\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 1.9251\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 1.9246\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 1.9233\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 1.9257\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 1.9243\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 1.9252\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 1.9256\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 1.9253\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 1.9239\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 1.9228\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 1.9239\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 1.9236\n",
            "Accuracy: 0.271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-V4bXncIkkp"
      },
      "source": [
        "En este caso, se alcanzó una precisión cercana al 27%.\n",
        "\n",
        "Ahora, se desarrollará un modelo combinado, capaz de realizar predicciones tanto de regresión como de clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkU1zCXJ9c-7"
      },
      "source": [
        "## 3. Combined Regression and Classification Models"
      ]
    }
  ]
}