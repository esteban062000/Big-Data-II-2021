{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRMHl02Y7vYh"
      },
      "source": [
        "# Neural Network Models for Combined Classification and Regression\n",
        "\n",
        "Algunos problemas de predicción requieren tanto valores numéricos como categóricos para la misma entrada de datos. Una manera de resolver estos problemas es utilizar modelos predictivos de regresión y clasificación, sobre los mismos datos, y utilizar los modelos secuencialmente.\n",
        "\n",
        "Otra forma (y, a menudo, más efectiva), es desarrollar una sola red neuronal que pueda predecir tanto un valor numérico como un categórico para la misma entrada. Esto se conoce como un **multi-output model** y puede ser relativamente fácil de desarrollar con las librerías **Keras** y **TensorFlow**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmDJ_-IE9OV-"
      },
      "source": [
        "## 1. Single Model for Regression and Classification\n",
        "\n",
        "Un problema de tener dos modelos distintos (uno para regresión y otro para clasificación), es que las predicciones pueden diverger entre ellos. Con un solo modelo, se tiene la ventaja de que se puede actualizar y mantener más fácilmente, ofreciendo mayor consistencia en las predicciones de ambos tipos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9MFiQWw9Rob"
      },
      "source": [
        "## 2. Separate Regresion and Classification Models\n",
        "\n",
        "En esta sección, se elegirá un *dataset* real donde se necesiten predicciones de regresión y clasificación al mismo tiempo, y luego se desarrollarán modelos separados para cada tipo de predicción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU9jVhia9VP3"
      },
      "source": [
        "### 2.a Abalone Dataset\n",
        "\n",
        "Este *dataset* describe los detalles físicos de *abalone* (abulón, tipo de molusco) y requiere predecir el número de anillos que posee, el cual es un aproximado de la edad de la creatura. La \"edad\" puede ser predicha como un valor numérico (en años) o una clase categórica (el año ordinal como una clase)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWBzYtVJASMs",
        "outputId": "94e9f447-bb56-4812-b766-abd4e1360f43"
      },
      "source": [
        "# load and summarize the abalone dataset\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "# summarize shape\n",
        "print(dataframe.shape)\n",
        "# summarize first few lines\n",
        "print(dataframe.head())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177, 9)\n",
            "   0      1      2      3       4       5       6      7   8\n",
            "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
            "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
            "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
            "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
            "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pfO-msAWU9"
      },
      "source": [
        "Se puede observar que existen 4177 ejemplos (filas) que se pueden utilizar para entrenar y evaluar el modelo y 9 *features* (columnas) incluyendo la variable de salida. Todas las variables de entrada, menos la primera, son numéricas. Para facilitar la preparación de datos, se eliminará la primera columna y se concentrará en modelar los valores numéricos de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whaOhUzW9YfB"
      },
      "source": [
        "### 2.b Regression Model\n",
        "\n",
        "Además de separar las columnas de entrada y salida, también se forzarán todas las columnas de entrada a que sean de tipo *float* (el esperado por las redes neuronales) y guardar la cantidad de *input features*, que será necesaria para construir el modelo más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUbaOaVBbM1"
      },
      "source": [
        "...\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elc7BLnvBdF9"
      },
      "source": [
        "Seguidamente, se puede dividir el *dataset* en el conjunto de entrenamiento y el conjunto de evaluación. Vamos a utilizar una muestra aleatoria de 67% del *dataset* para entrenar el modelo y el 33% restante para evaluarlo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT1kG98gBpVC"
      },
      "source": [
        "...\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBCJbrO_Bqdw"
      },
      "source": [
        "Seguidamente, se puede definir el modelo de un *multi layer perceptron*. Se tendrán dos capas ocultas, la primera con 20 nodos y la segunda con 10 nodos, ambas utilizando la función de activación ReLU y una inicialización de pesos *\"he normal\"* (una buena práctica). El número de capas y nodos fue elegido arbitrariamente. \n",
        "\n",
        "La capa de salida tendrá un solo nodo para predecir un valor numérico y una función de activación linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6eQX_c5CHIP"
      },
      "source": [
        "...\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5gse1bCH56"
      },
      "source": [
        "El modelo será entrenado para minimizar el MSE *loss function* utilizando la versión efectiva **Adam** del gradiente del descenso estocástico."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIPby5QRCS4v"
      },
      "source": [
        "...\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpzJroACT96"
      },
      "source": [
        "El modelo se entrenará durante 150 *epochs* y tendrá un *mini-batch size* de 32 muestras, nuevamente, parámetros elegidos arbitrariamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3utJih6DhiG"
      },
      "source": [
        "...\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfRNa35UDkXF"
      },
      "source": [
        "Finalmente, una vez el modelo ha sido entrenado, se evaluará y se reportará el MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMLmlzeXDp_J"
      },
      "source": [
        "...\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvohw00jDrsh"
      },
      "source": [
        "Al unir todas las partes, se tiene el siguiente código de una red neuronal MLP para el *dataset* tratado como un problema de regresión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAi3e1AJD2EW",
        "outputId": "3c721d9e-8778-4e98-9817-119fd28bb151"
      },
      "source": [
        "# regression mlp model for the abalone dataset\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# compile the keras model\n",
        "model.compile(loss='mse', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "error = mean_absolute_error(y_test, yhat)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 59.4564\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 12.1797\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 8.0399\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 7.5015\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 7.2487\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 7.0616\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 6.9186\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 6.8042\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 6.6513\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 6.4886\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 6.2426\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 6.0605\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 5.8669\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 5.7058\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 5.5454\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 5.3999\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 5.2980\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 5.1967\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 5.1850\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 5.1084\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 5.0649\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 5.0192\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 4.9980\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 4.9707\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 4.9920\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 4.9791\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 4.9400\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 4.9745\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 4.9323\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 4.9495\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 4.9594\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 4.9417\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 4.9469\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 4.9555\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 4.9177\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 4.9299\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 4.9124\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 4.9313\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 4.9054\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 4.9213\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 4.9093\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 4.9349\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 4.9191\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 4.9197\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 4.9406\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 4.9897\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 4.9356\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 4.8964\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 4.9502\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 4.9105\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 4.9079\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 4.8934\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 4.8943\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 4.9090\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 4.9021\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 4.8743\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 4.9405\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 4.8831\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 4.9338\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 4.9011\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 4.8899\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 4.8840\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 4.8707\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 4.8985\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 4.8934\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 4.8683\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 4.8959\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 4.9104\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 4.8718\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 4.8767\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 4.8926\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 4.9005\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 4.8706\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 4.8568\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 4.8858\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 4.8900\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 4.8521\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 4.8567\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 4.8484\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 4.8510\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 4.8711\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 4.8674\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 4.8227\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 4.8403\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 4.8293\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 4.8539\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 4.8477\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 4.8368\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 4.8488\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 4.8652\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 4.8611\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 4.8384\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 4.8227\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 4.8198\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 4.8100\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 4.8049\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 4.8272\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 4.8074\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 4.8213\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 4.8343\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 4.8199\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 4.8383\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 4.8141\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 4.8182\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 4.8183\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 4.7806\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 4.7890\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 4.8080\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 4.8177\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 4.7971\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 4.7987\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 4.7837\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 4.7878\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 4.7691\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 4.7950\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 4.7955\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 4.7837\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 4.7316\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 4.7778\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 4.7539\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 4.7419\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 4.7354\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 4.7313\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 4.7602\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 4.7573\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 4.7093\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 4.7270\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 4.7169\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 4.7326\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 4.6958\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 4.7425\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 4.7211\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 4.7019\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 4.6986\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 4.6973\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 4.6950\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 4.6920\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 4.7142\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 4.6946\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 4.6851\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 4.6798\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 4.6512\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 4.6886\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 4.6754\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 4.6612\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 4.6893\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 4.6684\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 4.6259\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 4.6485\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 4.6568\n",
            "MAE: 1.547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzQZLEQpD85i"
      },
      "source": [
        "Al correr el código se prepara el *dataset*, se entrena el modelo y se reporta un estimado de su error, en este caso, cercano a 1.5 (anillos).\n",
        "\n",
        "Ahora, veamos un modelo similar para clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-NhT7L9avb"
      },
      "source": [
        "### 2.c Classification Model\n",
        "\n",
        "El *abalone dataset* puede ser tomado como un problema de clasificación donde cada entero \"anillo\" es considerado como una clase categórica. El ejemplo es muy similar al anterior de regresión, con algunos pocos cambios.\n",
        "\n",
        "El primer cambio requiere asignar un entero separado para cada clase de \"anillo\", empezando en 0 y terminando en el número de clases menos 1. Esto se puede hacer con el **LabelEncoder**. También, se puede guardar el número total de clases, para utilizarla más adelante en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQQFiMYRGyNT"
      },
      "source": [
        "...\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtqZRQ6_GzYz"
      },
      "source": [
        "Luego de dividir los datos en los conjuntos de entrenamiento y evaluación, se puede definir el modelo y cambiar el número de *outputs* para que sea igual a la cantidad de clases y utilizar la función de activación *softmax*, común para problemas de clasificación multi clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ7ooyXaG_qx"
      },
      "source": [
        "...\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RlMjHP_HBIC"
      },
      "source": [
        "Ahora, se puede entrenar el modelo al minimizar la *sparse categorical cross-entropy function*, apropiada para problemas de clasificación multi clase con clases codificadas como enteros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KDeFs7HWgc"
      },
      "source": [
        "...\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TdXmHV4HXK6"
      },
      "source": [
        "Luego de que el modelo es entrenado, se puede evaluar su rendimiento al calcular la precisión en la clasificación con el conjunto de datos para evaluación, conocido como *hold-out set*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzi3aHVEHgzw"
      },
      "source": [
        "...\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKUkaargHkAo"
      },
      "source": [
        "Al unir todo esto, se tiene el siguietne código para un problema de clasificación sobre el *abalone dataset*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9B_ARg-HqFL",
        "outputId": "fb022ec0-6292-400e-dae3-e13cca24b8a4"
      },
      "source": [
        "# classification mlp model for the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# encode strings to integer\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y))\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# define the keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(n_class, activation='softmax'))\n",
        "# compile the keras model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)\n",
        "# evaluate on test set\n",
        "yhat = model.predict(X_test)\n",
        "yhat = argmax(yhat, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 3.3437\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 2.9312\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 2.5832\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 2.4424\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 2.3703\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 2.3202\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 2.2816\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 2.2515\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 2.2236\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 2.1992\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 2.1837\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 2.1729\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 2.1589\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 2.1518\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 2.1420\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 2.1341\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 2.1263\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 2.1213\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 2.1144\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 2.1071\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 2.1013\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 2.0954\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 2.0895\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 2.0828\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 2.0773\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 2.0721\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 2.0656\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 2.0595\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 2.0529\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 2.0483\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 2.0418\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 2.0375\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 2.0302\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 2.0271\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 2.0206\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 2.0155\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 2.0102\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 2.0070\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 2.0014\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 1.9974\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 1.9942\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 1.9919\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 1.9857\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 1.9847\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 1.9811\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 1.9774\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 1.9742\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 1.9722\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 1.9716\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 1.9663\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 1.9653\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 1.9656\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 1.9620\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 1.9603\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 1.9586\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 1.9572\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 1.9557\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 1.9567\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 1.9539\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 1.9533\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 1.9530\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 1.9494\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 1.9513\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 1.9482\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 1.9475\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 1.9448\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 1.9459\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 1.9469\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 1.9447\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 1.9442\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 1.9413\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 1.9427\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 1.9402\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 1.9414\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 1.9414\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 1.9421\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 1.9400\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 1.9397\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 1.9376\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 1.9388\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 1.9377\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 1.9368\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 1.9377\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 1.9360\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 1.9354\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 1.9350\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 1.9381\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 1.9369\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 1.9349\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 1.9352\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 1.9339\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 1.9334\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 1.9337\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 1.9324\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 1.9338\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 1.9320\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 1.9319\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 1.9319\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 1.9318\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 1.9321\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 1.9316\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 1.9323\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 1.9318\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 1.9312\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 1.9305\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 1.9288\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 1.9292\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 1.9307\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 1.9296\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 1.9296\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 1.9281\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 1.9290\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 1.9285\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 1.9268\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 1.9271\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 1.9278\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 1.9271\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 1.9277\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 1.9255\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 1.9295\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 1.9282\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 1.9262\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 1.9251\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 1.9273\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 1.9268\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 1.9269\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 1.9261\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 1.9277\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 1.9234\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 1.9251\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 1.9246\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 1.9264\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 1.9233\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 1.9257\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 1.9250\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 1.9243\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 1.9252\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 1.9256\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 1.9253\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 1.9239\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 1.9228\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 1.9239\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 1.9236\n",
            "Accuracy: 0.271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-V4bXncIkkp"
      },
      "source": [
        "En este caso, se alcanzó una precisión cercana al 27%.\n",
        "\n",
        "Ahora, se desarrollará un modelo combinado, capaz de realizar predicciones tanto de regresión como de clasificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkU1zCXJ9c-7"
      },
      "source": [
        "## 3. Combined Regression and Classification Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4CYctHkMp_m"
      },
      "source": [
        "Para este modelo *multi salida*, se pueden preparar los datos como se hizo para el problema de clasificación, pero guardando la variable objetivo codificada con un nombre separado para diferenciarla de los valores de la variable objetivo *raw*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt-HGcnNNdOa"
      },
      "source": [
        "...\n",
        "# encode strings to integer\n",
        "y_class = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE-sF36oNeWF"
      },
      "source": [
        "Luego, se pueden dividir los datos de entrada, *raw output* y variables codificadas de salida en conjuntos de entrenamiento y evaluación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM72j9r5NoA8"
      },
      "source": [
        "...\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP9oyDGMNpRl"
      },
      "source": [
        "Seguidamente, se puede definir el modelo utilizando el API de Keras. \n",
        "\n",
        "Se tomará la misma cantidad de entradas que anteriormente y también se utilizarán dos capas ocultas, configuradas de la misma manera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoi0j9c_OG23"
      },
      "source": [
        "...\n",
        "# input\n",
        "visible = Input(shape=(n_features,))\n",
        "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\n",
        "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85wIwdBMOIU-"
      },
      "source": [
        "Luego, se pueden definir dos capas de salida separadas que se conectan a la segunda capa oculta del modelo. \n",
        "\n",
        "La primera es una capa de salida de regresión que tiene un único nodo y una función de activación linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jegaE6KZOS9X"
      },
      "source": [
        "...\n",
        "# regression output\n",
        "out_reg = Dense(1, activation='linear')(hidden2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph8sHpk-O2AU"
      },
      "source": [
        "La segunda es una capa de salida de clasificación que tiene un nodo por cada clase y utiliza la función de activación *softmax*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMhejaAoO_Ti"
      },
      "source": [
        "...\n",
        "# classification output\n",
        "out_clas = Dense(n_class, activation='softmax')(hidden2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDOi3XTPPAj9"
      },
      "source": [
        "Seguidamente, se puede definir el modeloo con una sola capa de entrada y dos capas de salida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvakJjBiPE1L"
      },
      "source": [
        "...\n",
        "# define model\n",
        "model = Model(inputs=visible, outputs=[out_reg, out_clas])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndNLUKNOPHj5"
      },
      "source": [
        "Puesto que se tienen dos capas de salida, se puede compilar el modelo con dos *loss functions*, MSE para la primera capa (regresión) y *sparse categorical cross-entropy* para la segunda (clasificación)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhKNAE95PR-H"
      },
      "source": [
        "...\n",
        "# compile the keras model\n",
        "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fjA2RxwPX-e"
      },
      "source": [
        "También, se puede crear un gráfico del modelo como referencia. Para esto, se requiere que *pydot* y *pygraphviz* estén instalados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuQE7J9gPeKr"
      },
      "source": [
        "...\n",
        "# plot graph of model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxQQ1pKCPgRt"
      },
      "source": [
        "Cada vez que el modelo realiza una predicción, predice dos valores. Similarmente, cuando se entrena el modelo, necesitará una variable objetivo por muestra por cada salida. \n",
        "\n",
        "Por ende, se puede entrenar el modelo dándole cuidadosamente tanto el objetivo de regresión como el de clasificación a cada salida del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KWMpixkPxOP"
      },
      "source": [
        "...\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zLYVFkiPymi"
      },
      "source": [
        "El modelo entrenado se puede evaluar sobre el *hold-out set*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGEeRlloP2Du"
      },
      "source": [
        "...\n",
        "# make predictions on test set\n",
        "yhat1, yhat2 = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxXJkaDpP3i2"
      },
      "source": [
        "El primer arreglo se puede utilizar para evaluar las predicciones de la regresión mediante el MAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWt0i3l2P7Ol"
      },
      "source": [
        "...\n",
        "# calculate error for regression model\n",
        "error = mean_absolute_error(y_test, yhat1)\n",
        "print('MAE: %.3f' % error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8S842pvP9TE"
      },
      "source": [
        "El segundo arreglo se puede utilizar para evaluar las predicciones de clasificación mediante la precisión de las clasificaciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6O1a7nwQCZ7"
      },
      "source": [
        "...\n",
        "# evaluate accuracy for classification model\n",
        "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test_class, yhat2)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeyZDyswQEBK"
      },
      "source": [
        "Al unir todas las partes, se tiene el siguiente ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEa1Qaw0QH2m",
        "outputId": "320c1296-8a8e-4e2f-d636-892612dadcd7"
      },
      "source": [
        "# mlp for combined regression and classification predictions on the abalone dataset\n",
        "from numpy import unique\n",
        "from numpy import argmax\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "# load dataset\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'\n",
        "dataframe = read_csv(url, header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (y) variables\n",
        "X, y = dataset[:, 1:-1], dataset[:, -1]\n",
        "X, y = X.astype('float'), y.astype('float')\n",
        "n_features = X.shape[1]\n",
        "# encode strings to integer\n",
        "y_class = LabelEncoder().fit_transform(y)\n",
        "n_class = len(unique(y_class))\n",
        "# split data into train and test sets\n",
        "X_train, X_test, y_train, y_test, y_train_class, y_test_class = train_test_split(X, y, y_class, test_size=0.33, random_state=1)\n",
        "# input\n",
        "visible = Input(shape=(n_features,))\n",
        "hidden1 = Dense(20, activation='relu', kernel_initializer='he_normal')(visible)\n",
        "hidden2 = Dense(10, activation='relu', kernel_initializer='he_normal')(hidden1)\n",
        "# regression output\n",
        "out_reg = Dense(1, activation='linear')(hidden2)\n",
        "# classification output\n",
        "out_clas = Dense(n_class, activation='softmax')(hidden2)\n",
        "# define model\n",
        "model = Model(inputs=visible, outputs=[out_reg, out_clas])\n",
        "# compile the keras model\n",
        "model.compile(loss=['mse','sparse_categorical_crossentropy'], optimizer='adam')\n",
        "# plot graph of model\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit the keras model on the dataset\n",
        "model.fit(X_train, [y_train,y_train_class], epochs=150, batch_size=32, verbose=2)\n",
        "# make predictions on test set\n",
        "yhat1, yhat2 = model.predict(X_test)\n",
        "# calculate error for regression model\n",
        "error = mean_absolute_error(y_test, yhat1)\n",
        "print('MAE: %.3f' % error)\n",
        "# evaluate accuracy for classification model\n",
        "yhat2 = argmax(yhat2, axis=-1).astype('int')\n",
        "acc = accuracy_score(y_test_class, yhat2)\n",
        "print('Accuracy: %.3f' % acc)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "88/88 - 1s - loss: 73.7149 - dense_5_loss: 70.2362 - dense_6_loss: 3.4788\n",
            "Epoch 2/150\n",
            "88/88 - 0s - loss: 24.3119 - dense_5_loss: 21.2668 - dense_6_loss: 3.0451\n",
            "Epoch 3/150\n",
            "88/88 - 0s - loss: 11.0991 - dense_5_loss: 8.5374 - dense_6_loss: 2.5617\n",
            "Epoch 4/150\n",
            "88/88 - 0s - loss: 10.6892 - dense_5_loss: 8.1726 - dense_6_loss: 2.5166\n",
            "Epoch 5/150\n",
            "88/88 - 0s - loss: 10.3346 - dense_5_loss: 7.8252 - dense_6_loss: 2.5094\n",
            "Epoch 6/150\n",
            "88/88 - 0s - loss: 10.0126 - dense_5_loss: 7.5073 - dense_6_loss: 2.5053\n",
            "Epoch 7/150\n",
            "88/88 - 0s - loss: 9.7598 - dense_5_loss: 7.2565 - dense_6_loss: 2.5033\n",
            "Epoch 8/150\n",
            "88/88 - 0s - loss: 9.5369 - dense_5_loss: 7.0358 - dense_6_loss: 2.5011\n",
            "Epoch 9/150\n",
            "88/88 - 0s - loss: 9.3708 - dense_5_loss: 6.8707 - dense_6_loss: 2.5001\n",
            "Epoch 10/150\n",
            "88/88 - 0s - loss: 9.2362 - dense_5_loss: 6.7380 - dense_6_loss: 2.4982\n",
            "Epoch 11/150\n",
            "88/88 - 0s - loss: 9.1370 - dense_5_loss: 6.6409 - dense_6_loss: 2.4960\n",
            "Epoch 12/150\n",
            "88/88 - 0s - loss: 9.0528 - dense_5_loss: 6.5660 - dense_6_loss: 2.4868\n",
            "Epoch 13/150\n",
            "88/88 - 0s - loss: 8.9083 - dense_5_loss: 6.4400 - dense_6_loss: 2.4683\n",
            "Epoch 14/150\n",
            "88/88 - 0s - loss: 8.6646 - dense_5_loss: 6.2204 - dense_6_loss: 2.4441\n",
            "Epoch 15/150\n",
            "88/88 - 0s - loss: 8.4841 - dense_5_loss: 6.0634 - dense_6_loss: 2.4207\n",
            "Epoch 16/150\n",
            "88/88 - 0s - loss: 8.3654 - dense_5_loss: 5.9651 - dense_6_loss: 2.4003\n",
            "Epoch 17/150\n",
            "88/88 - 0s - loss: 8.2205 - dense_5_loss: 5.8419 - dense_6_loss: 2.3786\n",
            "Epoch 18/150\n",
            "88/88 - 0s - loss: 8.0912 - dense_5_loss: 5.7270 - dense_6_loss: 2.3642\n",
            "Epoch 19/150\n",
            "88/88 - 0s - loss: 8.0347 - dense_5_loss: 5.6823 - dense_6_loss: 2.3523\n",
            "Epoch 20/150\n",
            "88/88 - 0s - loss: 7.8903 - dense_5_loss: 5.5518 - dense_6_loss: 2.3385\n",
            "Epoch 21/150\n",
            "88/88 - 0s - loss: 7.8041 - dense_5_loss: 5.4814 - dense_6_loss: 2.3227\n",
            "Epoch 22/150\n",
            "88/88 - 0s - loss: 7.7257 - dense_5_loss: 5.4152 - dense_6_loss: 2.3105\n",
            "Epoch 23/150\n",
            "88/88 - 0s - loss: 7.6321 - dense_5_loss: 5.3334 - dense_6_loss: 2.2988\n",
            "Epoch 24/150\n",
            "88/88 - 0s - loss: 7.5429 - dense_5_loss: 5.2575 - dense_6_loss: 2.2854\n",
            "Epoch 25/150\n",
            "88/88 - 0s - loss: 7.5092 - dense_5_loss: 5.2326 - dense_6_loss: 2.2766\n",
            "Epoch 26/150\n",
            "88/88 - 0s - loss: 7.4316 - dense_5_loss: 5.1672 - dense_6_loss: 2.2645\n",
            "Epoch 27/150\n",
            "88/88 - 0s - loss: 7.4158 - dense_5_loss: 5.1585 - dense_6_loss: 2.2573\n",
            "Epoch 28/150\n",
            "88/88 - 0s - loss: 7.3657 - dense_5_loss: 5.1193 - dense_6_loss: 2.2464\n",
            "Epoch 29/150\n",
            "88/88 - 0s - loss: 7.2979 - dense_5_loss: 5.0635 - dense_6_loss: 2.2345\n",
            "Epoch 30/150\n",
            "88/88 - 0s - loss: 7.2912 - dense_5_loss: 5.0635 - dense_6_loss: 2.2277\n",
            "Epoch 31/150\n",
            "88/88 - 0s - loss: 7.2402 - dense_5_loss: 5.0215 - dense_6_loss: 2.2186\n",
            "Epoch 32/150\n",
            "88/88 - 0s - loss: 7.2047 - dense_5_loss: 4.9939 - dense_6_loss: 2.2108\n",
            "Epoch 33/150\n",
            "88/88 - 0s - loss: 7.1983 - dense_5_loss: 4.9924 - dense_6_loss: 2.2060\n",
            "Epoch 34/150\n",
            "88/88 - 0s - loss: 7.1811 - dense_5_loss: 4.9800 - dense_6_loss: 2.2011\n",
            "Epoch 35/150\n",
            "88/88 - 0s - loss: 7.1572 - dense_5_loss: 4.9656 - dense_6_loss: 2.1916\n",
            "Epoch 36/150\n",
            "88/88 - 0s - loss: 7.1618 - dense_5_loss: 4.9720 - dense_6_loss: 2.1897\n",
            "Epoch 37/150\n",
            "88/88 - 0s - loss: 7.1315 - dense_5_loss: 4.9502 - dense_6_loss: 2.1814\n",
            "Epoch 38/150\n",
            "88/88 - 0s - loss: 7.1210 - dense_5_loss: 4.9424 - dense_6_loss: 2.1786\n",
            "Epoch 39/150\n",
            "88/88 - 0s - loss: 7.1468 - dense_5_loss: 4.9706 - dense_6_loss: 2.1762\n",
            "Epoch 40/150\n",
            "88/88 - 0s - loss: 7.1362 - dense_5_loss: 4.9671 - dense_6_loss: 2.1692\n",
            "Epoch 41/150\n",
            "88/88 - 0s - loss: 7.1173 - dense_5_loss: 4.9496 - dense_6_loss: 2.1678\n",
            "Epoch 42/150\n",
            "88/88 - 0s - loss: 7.1017 - dense_5_loss: 4.9384 - dense_6_loss: 2.1634\n",
            "Epoch 43/150\n",
            "88/88 - 0s - loss: 7.0960 - dense_5_loss: 4.9351 - dense_6_loss: 2.1610\n",
            "Epoch 44/150\n",
            "88/88 - 0s - loss: 7.0852 - dense_5_loss: 4.9318 - dense_6_loss: 2.1534\n",
            "Epoch 45/150\n",
            "88/88 - 0s - loss: 7.0810 - dense_5_loss: 4.9279 - dense_6_loss: 2.1530\n",
            "Epoch 46/150\n",
            "88/88 - 0s - loss: 7.0681 - dense_5_loss: 4.9206 - dense_6_loss: 2.1475\n",
            "Epoch 47/150\n",
            "88/88 - 0s - loss: 7.0634 - dense_5_loss: 4.9171 - dense_6_loss: 2.1463\n",
            "Epoch 48/150\n",
            "88/88 - 0s - loss: 7.0624 - dense_5_loss: 4.9203 - dense_6_loss: 2.1421\n",
            "Epoch 49/150\n",
            "88/88 - 0s - loss: 7.0721 - dense_5_loss: 4.9308 - dense_6_loss: 2.1413\n",
            "Epoch 50/150\n",
            "88/88 - 0s - loss: 7.0449 - dense_5_loss: 4.9092 - dense_6_loss: 2.1358\n",
            "Epoch 51/150\n",
            "88/88 - 0s - loss: 7.0390 - dense_5_loss: 4.9048 - dense_6_loss: 2.1342\n",
            "Epoch 52/150\n",
            "88/88 - 0s - loss: 7.0313 - dense_5_loss: 4.8997 - dense_6_loss: 2.1316\n",
            "Epoch 53/150\n",
            "88/88 - 0s - loss: 7.0382 - dense_5_loss: 4.9077 - dense_6_loss: 2.1305\n",
            "Epoch 54/150\n",
            "88/88 - 0s - loss: 7.0378 - dense_5_loss: 4.9110 - dense_6_loss: 2.1268\n",
            "Epoch 55/150\n",
            "88/88 - 0s - loss: 7.0582 - dense_5_loss: 4.9309 - dense_6_loss: 2.1273\n",
            "Epoch 56/150\n",
            "88/88 - 0s - loss: 7.0164 - dense_5_loss: 4.8948 - dense_6_loss: 2.1216\n",
            "Epoch 57/150\n",
            "88/88 - 0s - loss: 7.0136 - dense_5_loss: 4.8946 - dense_6_loss: 2.1191\n",
            "Epoch 58/150\n",
            "88/88 - 0s - loss: 7.0189 - dense_5_loss: 4.9020 - dense_6_loss: 2.1169\n",
            "Epoch 59/150\n",
            "88/88 - 0s - loss: 6.9922 - dense_5_loss: 4.8770 - dense_6_loss: 2.1152\n",
            "Epoch 60/150\n",
            "88/88 - 0s - loss: 6.9931 - dense_5_loss: 4.8786 - dense_6_loss: 2.1145\n",
            "Epoch 61/150\n",
            "88/88 - 0s - loss: 7.0009 - dense_5_loss: 4.8895 - dense_6_loss: 2.1115\n",
            "Epoch 62/150\n",
            "88/88 - 0s - loss: 7.0266 - dense_5_loss: 4.9171 - dense_6_loss: 2.1095\n",
            "Epoch 63/150\n",
            "88/88 - 0s - loss: 6.9854 - dense_5_loss: 4.8803 - dense_6_loss: 2.1051\n",
            "Epoch 64/150\n",
            "88/88 - 0s - loss: 6.9978 - dense_5_loss: 4.8960 - dense_6_loss: 2.1017\n",
            "Epoch 65/150\n",
            "88/88 - 0s - loss: 6.9606 - dense_5_loss: 4.8635 - dense_6_loss: 2.0971\n",
            "Epoch 66/150\n",
            "88/88 - 0s - loss: 6.9390 - dense_5_loss: 4.8455 - dense_6_loss: 2.0935\n",
            "Epoch 67/150\n",
            "88/88 - 0s - loss: 6.9502 - dense_5_loss: 4.8611 - dense_6_loss: 2.0892\n",
            "Epoch 68/150\n",
            "88/88 - 0s - loss: 7.0059 - dense_5_loss: 4.9169 - dense_6_loss: 2.0890\n",
            "Epoch 69/150\n",
            "88/88 - 0s - loss: 6.9337 - dense_5_loss: 4.8504 - dense_6_loss: 2.0833\n",
            "Epoch 70/150\n",
            "88/88 - 0s - loss: 6.9204 - dense_5_loss: 4.8424 - dense_6_loss: 2.0779\n",
            "Epoch 71/150\n",
            "88/88 - 0s - loss: 6.9216 - dense_5_loss: 4.8461 - dense_6_loss: 2.0755\n",
            "Epoch 72/150\n",
            "88/88 - 0s - loss: 6.9431 - dense_5_loss: 4.8694 - dense_6_loss: 2.0737\n",
            "Epoch 73/150\n",
            "88/88 - 0s - loss: 6.9204 - dense_5_loss: 4.8505 - dense_6_loss: 2.0700\n",
            "Epoch 74/150\n",
            "88/88 - 0s - loss: 6.9343 - dense_5_loss: 4.8655 - dense_6_loss: 2.0688\n",
            "Epoch 75/150\n",
            "88/88 - 0s - loss: 6.9021 - dense_5_loss: 4.8379 - dense_6_loss: 2.0642\n",
            "Epoch 76/150\n",
            "88/88 - 0s - loss: 6.8788 - dense_5_loss: 4.8192 - dense_6_loss: 2.0596\n",
            "Epoch 77/150\n",
            "88/88 - 0s - loss: 6.9129 - dense_5_loss: 4.8527 - dense_6_loss: 2.0601\n",
            "Epoch 78/150\n",
            "88/88 - 0s - loss: 6.8579 - dense_5_loss: 4.8036 - dense_6_loss: 2.0543\n",
            "Epoch 79/150\n",
            "88/88 - 0s - loss: 6.8729 - dense_5_loss: 4.8217 - dense_6_loss: 2.0513\n",
            "Epoch 80/150\n",
            "88/88 - 0s - loss: 6.8496 - dense_5_loss: 4.8004 - dense_6_loss: 2.0492\n",
            "Epoch 81/150\n",
            "88/88 - 0s - loss: 6.8517 - dense_5_loss: 4.8037 - dense_6_loss: 2.0479\n",
            "Epoch 82/150\n",
            "88/88 - 0s - loss: 6.8404 - dense_5_loss: 4.7948 - dense_6_loss: 2.0457\n",
            "Epoch 83/150\n",
            "88/88 - 0s - loss: 6.8264 - dense_5_loss: 4.7851 - dense_6_loss: 2.0414\n",
            "Epoch 84/150\n",
            "88/88 - 0s - loss: 6.8244 - dense_5_loss: 4.7849 - dense_6_loss: 2.0394\n",
            "Epoch 85/150\n",
            "88/88 - 0s - loss: 6.7984 - dense_5_loss: 4.7632 - dense_6_loss: 2.0352\n",
            "Epoch 86/150\n",
            "88/88 - 0s - loss: 6.8114 - dense_5_loss: 4.7766 - dense_6_loss: 2.0348\n",
            "Epoch 87/150\n",
            "88/88 - 0s - loss: 6.7973 - dense_5_loss: 4.7657 - dense_6_loss: 2.0316\n",
            "Epoch 88/150\n",
            "88/88 - 0s - loss: 6.7997 - dense_5_loss: 4.7699 - dense_6_loss: 2.0299\n",
            "Epoch 89/150\n",
            "88/88 - 0s - loss: 6.8164 - dense_5_loss: 4.7870 - dense_6_loss: 2.0294\n",
            "Epoch 90/150\n",
            "88/88 - 0s - loss: 6.8106 - dense_5_loss: 4.7851 - dense_6_loss: 2.0255\n",
            "Epoch 91/150\n",
            "88/88 - 0s - loss: 6.8003 - dense_5_loss: 4.7744 - dense_6_loss: 2.0259\n",
            "Epoch 92/150\n",
            "88/88 - 0s - loss: 6.8143 - dense_5_loss: 4.7928 - dense_6_loss: 2.0216\n",
            "Epoch 93/150\n",
            "88/88 - 0s - loss: 6.7733 - dense_5_loss: 4.7550 - dense_6_loss: 2.0183\n",
            "Epoch 94/150\n",
            "88/88 - 0s - loss: 6.8035 - dense_5_loss: 4.7826 - dense_6_loss: 2.0209\n",
            "Epoch 95/150\n",
            "88/88 - 0s - loss: 6.7875 - dense_5_loss: 4.7691 - dense_6_loss: 2.0184\n",
            "Epoch 96/150\n",
            "88/88 - 0s - loss: 6.7748 - dense_5_loss: 4.7597 - dense_6_loss: 2.0151\n",
            "Epoch 97/150\n",
            "88/88 - 0s - loss: 6.7664 - dense_5_loss: 4.7551 - dense_6_loss: 2.0113\n",
            "Epoch 98/150\n",
            "88/88 - 0s - loss: 6.7632 - dense_5_loss: 4.7521 - dense_6_loss: 2.0112\n",
            "Epoch 99/150\n",
            "88/88 - 0s - loss: 6.7506 - dense_5_loss: 4.7414 - dense_6_loss: 2.0092\n",
            "Epoch 100/150\n",
            "88/88 - 0s - loss: 6.7736 - dense_5_loss: 4.7646 - dense_6_loss: 2.0091\n",
            "Epoch 101/150\n",
            "88/88 - 0s - loss: 6.7521 - dense_5_loss: 4.7442 - dense_6_loss: 2.0079\n",
            "Epoch 102/150\n",
            "88/88 - 0s - loss: 6.7581 - dense_5_loss: 4.7539 - dense_6_loss: 2.0041\n",
            "Epoch 103/150\n",
            "88/88 - 0s - loss: 6.7467 - dense_5_loss: 4.7442 - dense_6_loss: 2.0025\n",
            "Epoch 104/150\n",
            "88/88 - 0s - loss: 6.7448 - dense_5_loss: 4.7427 - dense_6_loss: 2.0021\n",
            "Epoch 105/150\n",
            "88/88 - 0s - loss: 6.7452 - dense_5_loss: 4.7445 - dense_6_loss: 2.0007\n",
            "Epoch 106/150\n",
            "88/88 - 0s - loss: 6.7622 - dense_5_loss: 4.7609 - dense_6_loss: 2.0013\n",
            "Epoch 107/150\n",
            "88/88 - 0s - loss: 6.7657 - dense_5_loss: 4.7630 - dense_6_loss: 2.0027\n",
            "Epoch 108/150\n",
            "88/88 - 0s - loss: 6.7646 - dense_5_loss: 4.7669 - dense_6_loss: 1.9977\n",
            "Epoch 109/150\n",
            "88/88 - 0s - loss: 6.7401 - dense_5_loss: 4.7442 - dense_6_loss: 1.9959\n",
            "Epoch 110/150\n",
            "88/88 - 0s - loss: 6.7341 - dense_5_loss: 4.7397 - dense_6_loss: 1.9944\n",
            "Epoch 111/150\n",
            "88/88 - 0s - loss: 6.7106 - dense_5_loss: 4.7164 - dense_6_loss: 1.9942\n",
            "Epoch 112/150\n",
            "88/88 - 0s - loss: 6.7250 - dense_5_loss: 4.7316 - dense_6_loss: 1.9933\n",
            "Epoch 113/150\n",
            "88/88 - 0s - loss: 6.7307 - dense_5_loss: 4.7377 - dense_6_loss: 1.9930\n",
            "Epoch 114/150\n",
            "88/88 - 0s - loss: 6.7016 - dense_5_loss: 4.7121 - dense_6_loss: 1.9895\n",
            "Epoch 115/150\n",
            "88/88 - 0s - loss: 6.7314 - dense_5_loss: 4.7393 - dense_6_loss: 1.9921\n",
            "Epoch 116/150\n",
            "88/88 - 0s - loss: 6.7231 - dense_5_loss: 4.7347 - dense_6_loss: 1.9884\n",
            "Epoch 117/150\n",
            "88/88 - 0s - loss: 6.7234 - dense_5_loss: 4.7323 - dense_6_loss: 1.9911\n",
            "Epoch 118/150\n",
            "88/88 - 0s - loss: 6.7042 - dense_5_loss: 4.7151 - dense_6_loss: 1.9891\n",
            "Epoch 119/150\n",
            "88/88 - 0s - loss: 6.7159 - dense_5_loss: 4.7295 - dense_6_loss: 1.9864\n",
            "Epoch 120/150\n",
            "88/88 - 0s - loss: 6.7131 - dense_5_loss: 4.7272 - dense_6_loss: 1.9858\n",
            "Epoch 121/150\n",
            "88/88 - 0s - loss: 6.6996 - dense_5_loss: 4.7169 - dense_6_loss: 1.9827\n",
            "Epoch 122/150\n",
            "88/88 - 0s - loss: 6.7240 - dense_5_loss: 4.7382 - dense_6_loss: 1.9858\n",
            "Epoch 123/150\n",
            "88/88 - 0s - loss: 6.6971 - dense_5_loss: 4.7128 - dense_6_loss: 1.9843\n",
            "Epoch 124/150\n",
            "88/88 - 0s - loss: 6.6882 - dense_5_loss: 4.7076 - dense_6_loss: 1.9806\n",
            "Epoch 125/150\n",
            "88/88 - 0s - loss: 6.6791 - dense_5_loss: 4.6981 - dense_6_loss: 1.9810\n",
            "Epoch 126/150\n",
            "88/88 - 0s - loss: 6.6720 - dense_5_loss: 4.6928 - dense_6_loss: 1.9791\n",
            "Epoch 127/150\n",
            "88/88 - 0s - loss: 6.7200 - dense_5_loss: 4.7367 - dense_6_loss: 1.9833\n",
            "Epoch 128/150\n",
            "88/88 - 0s - loss: 6.6975 - dense_5_loss: 4.7152 - dense_6_loss: 1.9823\n",
            "Epoch 129/150\n",
            "88/88 - 0s - loss: 6.6884 - dense_5_loss: 4.7102 - dense_6_loss: 1.9782\n",
            "Epoch 130/150\n",
            "88/88 - 0s - loss: 6.6972 - dense_5_loss: 4.7187 - dense_6_loss: 1.9785\n",
            "Epoch 131/150\n",
            "88/88 - 0s - loss: 6.6969 - dense_5_loss: 4.7174 - dense_6_loss: 1.9795\n",
            "Epoch 132/150\n",
            "88/88 - 0s - loss: 6.6691 - dense_5_loss: 4.6930 - dense_6_loss: 1.9762\n",
            "Epoch 133/150\n",
            "88/88 - 0s - loss: 6.7044 - dense_5_loss: 4.7265 - dense_6_loss: 1.9779\n",
            "Epoch 134/150\n",
            "88/88 - 0s - loss: 6.6879 - dense_5_loss: 4.7129 - dense_6_loss: 1.9750\n",
            "Epoch 135/150\n",
            "88/88 - 0s - loss: 6.6803 - dense_5_loss: 4.7057 - dense_6_loss: 1.9746\n",
            "Epoch 136/150\n",
            "88/88 - 0s - loss: 6.6884 - dense_5_loss: 4.7136 - dense_6_loss: 1.9748\n",
            "Epoch 137/150\n",
            "88/88 - 0s - loss: 6.6759 - dense_5_loss: 4.7021 - dense_6_loss: 1.9738\n",
            "Epoch 138/150\n",
            "88/88 - 0s - loss: 6.6782 - dense_5_loss: 4.7052 - dense_6_loss: 1.9730\n",
            "Epoch 139/150\n",
            "88/88 - 0s - loss: 6.6772 - dense_5_loss: 4.7030 - dense_6_loss: 1.9742\n",
            "Epoch 140/150\n",
            "88/88 - 0s - loss: 6.6560 - dense_5_loss: 4.6831 - dense_6_loss: 1.9729\n",
            "Epoch 141/150\n",
            "88/88 - 0s - loss: 6.6801 - dense_5_loss: 4.7065 - dense_6_loss: 1.9736\n",
            "Epoch 142/150\n",
            "88/88 - 0s - loss: 6.7206 - dense_5_loss: 4.7453 - dense_6_loss: 1.9753\n",
            "Epoch 143/150\n",
            "88/88 - 0s - loss: 6.6553 - dense_5_loss: 4.6870 - dense_6_loss: 1.9682\n",
            "Epoch 144/150\n",
            "88/88 - 0s - loss: 6.6706 - dense_5_loss: 4.6968 - dense_6_loss: 1.9737\n",
            "Epoch 145/150\n",
            "88/88 - 0s - loss: 6.6587 - dense_5_loss: 4.6886 - dense_6_loss: 1.9701\n",
            "Epoch 146/150\n",
            "88/88 - 0s - loss: 6.6568 - dense_5_loss: 4.6854 - dense_6_loss: 1.9714\n",
            "Epoch 147/150\n",
            "88/88 - 0s - loss: 6.6518 - dense_5_loss: 4.6832 - dense_6_loss: 1.9686\n",
            "Epoch 148/150\n",
            "88/88 - 0s - loss: 6.6431 - dense_5_loss: 4.6761 - dense_6_loss: 1.9671\n",
            "Epoch 149/150\n",
            "88/88 - 0s - loss: 6.6628 - dense_5_loss: 4.6936 - dense_6_loss: 1.9691\n",
            "Epoch 150/150\n",
            "88/88 - 0s - loss: 6.6385 - dense_5_loss: 4.6723 - dense_6_loss: 1.9661\n",
            "MAE: 1.559\n",
            "Accuracy: 0.274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-BHTREfQO2t"
      },
      "source": [
        "En este caso, se alcanzó un error razonable de 1.56 (anillos) y una precisión de 27%, similar a las anteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbm99BOJQjmH"
      },
      "source": [
        "# Resumen del tema tratado\n",
        "\n",
        "Este tutorial combinó los modelos de regresión y clasificación mostrados anteriormente, para utilizarlos en conjunto en un único modelo para resolver un problema que involucra ambos tipos. \n",
        "\n",
        "Primeramente, se mostraron los modelos por separado, con el fin de adquirir una primera impresión sobre cómo resuelven el problema y, además, para conocer cuál es el rendimiento de ellos por separado. Seguidamente, se construyó el modelo combinado y se constató que sus resultados fueron similares a los alcanzados por los modelos ejecutados individualmente. \n",
        "\n",
        "Durante el tutorial, se señalaron aquellas diferencias en el proceso de la combinación de modelos, por ejemplo, el cuidado necesario para procesar la entrada de datos (y separar correctamente las variables) y al determinar cuáles partes del *dataset* serán utilizadas por cada capa.\n",
        "\n",
        "# Comentarios sobre algo que haya aprendido\n",
        "\n",
        "Me gustó ver una aplicación un poco más compleja de las redes neuronales. De haber tenido que resolver este problema antes, habría construido dos modelos por separado, pues no sabía que se podía realizar todo en una misma red. La manera de explicar cada parte del código facilita mucho su comprensión.\n",
        "\n",
        "También, considero que fue un buen repaso sobre los conceptos clave de las redes neuronales, por ejemplo, cuando se hicieron comentarios sobre utilizar ReLU para regresión y *softmax* para clasificación. En lo personal, considero que estos ejemplos prácticos ayudan a terminar de forjar el conocimiento teórico adquirido anteriormente.\n",
        "\n",
        "Finalmente, me gustó el enfoque del tutorial: al construir los modelos por separado primero, se tuvo una medida para comparar el resultado de combinarlos y poder constatar, con datos reales, que el modelo combinado es tan bueno como los modelos individuales, con todas las ventajas que significa.\n",
        "\n",
        "# Dudas sobre el tema\n",
        "\n",
        "No tengo dudas sobre el tema."
      ]
    }
  ]
}