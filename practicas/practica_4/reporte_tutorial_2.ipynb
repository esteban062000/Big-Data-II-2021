{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY0Q5Xo6Ecn7"
      },
      "source": [
        "# Extreme Gradient Boosting (XGBoost) Ensemble in Python\n",
        "\n",
        "**Extreme Gradient Boosting (XGBoost)** es una librería *open-source* que provee una implementación efectiva y eficiente del algoritmo *gradient boosting*.\n",
        "\n",
        "Poco después de su desarrollo, XGBoost se convirtió en el método *go-to* y, a menudo, el componente clave en las soluciones ganadoras en competencias de problemas de clasificación y regresión.\n",
        "\n",
        "## 1. Extreme Gradient Boosting Algorithm\n",
        "\n",
        "**Gradient boosting** se refiere a una clase de algoritmos de *machine learning* que pueden ser utilizados para problemas de clasificación o regresión.\n",
        "\n",
        "Los *ensembles* son construidos a partir de árboles de decisión. Los árboles se agregan uno por uno al *ensemble* y son entrenados para corregir errores en las predicciones realizadas por modelos anteriores. Este es un tipo de *ensemble machine learning model* conocido como **boosting**.\n",
        "\n",
        "Los modelos son entrenados utilizando cualquier función de pérdida arbitrariamente diferenciable y el algoritmo de optimización *gradient descent*. Esto le da a la técnica su nombre, *gradient boosting*, puesto qe la pérdida del gradiente es minimizada conforme el modelo es entrenado, similar a una red neuronal.\n",
        "\n",
        "Las dos razones principales para utilizar XGBoost son la rapidez de ejcución y el rendimiento del modelo. XGBoost domina los *datasets* estructurados o tabulados en problemas predictivos de clasificación o regresión. Una prueba de esto es que es el algoritmo *go-to* de los ganadores de competencias en Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkpnqAhHE9de"
      },
      "source": [
        "## 2. XGBoost Scikit-Learn API\n",
        "\n",
        "XGBoost puede ser instalado como una librería *standalone* y un modelo XGBoost puede ser desarrollado utilizando el API *scikit-learn*. El primer paso es instalar la librería XGBoost, mediante el pip python manager:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK6rbQ6gJz32"
      },
      "source": [
        "sudo pip install xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PWxzhQtJl04"
      },
      "source": [
        "Se puede confirmar si se instaló correctamente con el siguiente script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_W8NhaMJlSk",
        "outputId": "e1b9134f-04ef-4d52-8804-a14dd72d840d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check xgboost version\n",
        "import xgboost\n",
        "print(xgboost.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yKGim26KAa_"
      },
      "source": [
        "La librería XGBoost tiene su propio API, pero en este tutorial, vamos a usar el método a través de las *wrapper classes* de *scikit-learn*: **XGBRegressor** y **XGBClassifier**. Se utiliza aleatoriedad en la construcción del modelo, por lo que en cada ejecución, se tendrá una salida ligeramente diferente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUzko77-FEbo"
      },
      "source": [
        "### a. XGBoost Ensemble for Classification\n",
        "\n",
        "Primeramente, se puede invocar a la función **make_classification()** para crear un *synthetic binary classification problem* con 1000 ejemplos y 20 variables de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuIdYeFCKm3K",
        "outputId": "9a167819-c351-49df-9cb1-4b9cb9efd452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3gFXKBNKo3b"
      },
      "source": [
        "Seguidamente, se evalúa el modelo utilizando **repeated stratified k-fold cross-validation**, con 3 *repeats* y 10 *folds*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZkqb7NK0rV",
        "outputId": "e3083659-38c9-4e53-e5d5-2a56f3e88d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# evaluate xgboost algorithm for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
        "# define the model\n",
        "model = XGBClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.899 (0.029)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-SafUiVK4Fq"
      },
      "source": [
        "En este caso, el *XGBoost ensemble*, con hiperparámetros por defecto, alcanzó una precisión en la clasificación cercana al 90%. \n",
        "\n",
        "También, se pueden realizar predicciones con la función **predict()**. Esta función espera siempre un arreglo NumPy como una matriz con una fila por cada variable de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiVsCkd6LMWB",
        "outputId": "a5fce70a-d70f-4c1f-9e8f-d5d10285c88d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make predictions using xgboost for classification\n",
        "from numpy import asarray\n",
        "from sklearn.datasets import make_classification\n",
        "from xgboost import XGBClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
        "# define the model\n",
        "model = XGBClassifier()\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
        "row = asarray([row])\n",
        "yhat = model.predict(row)\n",
        "print('Predicted Class: %d' % yhat[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjNvR7wTFI00"
      },
      "source": [
        "### b. XGBoost Ensemble for Regression\n",
        "\n",
        "Nuevamente, se puede utilizar la función **make_regresion()** para crear un *synthetic regression problem* con 1000 ejemplos y 20 variables de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eOZZRvhLbAi",
        "outputId": "70c7e65e-f079-4732-d651-3ec4602220cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiXety9vLcvd"
      },
      "source": [
        "Luego, se evalúa el modelo utilizando **repeated k-fold cross-validation**, con 3 *repeats* y 10 *folds*. Se reportará el mean absolute error (MAE) del modelo a lo largo de todas las repeats y folds. La librería scikit-learn toma el MAE negativo, por lo que es maximizado en lugar de minimizado. Esto significa que los MAE más negativos son mejores, y un modelo perfecto tiene un MAE de 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJmw_W1oLuyr",
        "outputId": "c3cd92a7-1e02-4b05-ca7a-ee35e73145fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# evaluate xgboost ensemble for regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from xgboost import XGBRegressor\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
        "# define the model\n",
        "model = XGBRegressor()\n",
        "# evaluate the model\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: -62.762 (3.219)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAi1BjeQL2PC"
      },
      "source": [
        "En este caso, el *XGBoost ensemble* con hiperparámetros por defecto alcanza un MAE cercano a 63.\n",
        "\n",
        "También, se pueden realizar predicciones con la función **predict()**, que espera un arreglo NumPy como parámetro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPeDgFURMEsq",
        "outputId": "478369ad-b7ec-4979-b622-b61f89ffb83f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# gradient xgboost for making predictions for regression\n",
        "from numpy import asarray\n",
        "from sklearn.datasets import make_regression\n",
        "from xgboost import XGBRegressor\n",
        "# define dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
        "# define the model\n",
        "model = XGBRegressor()\n",
        "# fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "# make a single prediction\n",
        "row = [0.20543991,-0.97049844,-0.81403429,-0.23842689,-0.60704084,-0.48541492,0.53113006,2.01834338,-0.90745243,-1.85859731,-1.02334791,-0.6877744,0.60984819,-0.70630121,-1.29161497,1.32385441,1.42150747,1.26567231,2.56569098,-0.11154792]\n",
        "row = asarray([row])\n",
        "yhat = model.predict(row)\n",
        "print('Prediction: %d' % yhat[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16:57:09] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Prediction: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBcW9_gbFMBd"
      },
      "source": [
        "## 3. XGBoost Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R20lUCDFT0S"
      },
      "source": [
        "### a. Explore Number of Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqFrGd0aFWKC"
      },
      "source": [
        "### b. Explore Tree Depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejqsUW2oFX69"
      },
      "source": [
        "### c. Explore Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weHVcmhTFZ1R"
      },
      "source": [
        "### d. Explore Number of Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXjFMeL8Fbsg"
      },
      "source": [
        "### e. Explore Number of Features"
      ]
    }
  ]
}