{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9w4T9zJEM0e"
      },
      "source": [
        "# How to Develop a Random Forest Ensemble in Python\n",
        "\n",
        "> Random forest is an ensemble machine learning algorithm.\n",
        "\n",
        "Es tal vez el algoritmo de *machine learning* más popular y ampliamente usado dado su buen o excelente rendimiento, a lo largo de varios problemas de clasificación y regresión predictiva. También, es sencillo de usar, pues solo tiene algunos hiperparámetros clave y heurísticas sensibles para configurar estos parámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QPoHxY_GjAu"
      },
      "source": [
        "## 1. Random Forest Algorithm\n",
        "\n",
        "> Random forest is an ensemble of decision tree algorithms.\n",
        "\n",
        "Es una extensión de **bootstrap aggregation (bagging)** de los árboles de decisión y puede ser utilizado para problemas de clasificación y regresión.\n",
        "\n",
        "En *bagging*, un número de árboles de decisión son creados, donde cada árbol se crea a partir de una muestra *bootstrap* diferente del *dataset*. Una muestra *bootstrap* es una muestra del *dataset* donde una muestra puede aparecer más de una vez, conocico como **muestreo con reemplazo**.\n",
        "\n",
        "*Bagging* es un algoritmo de *ensemble* efectivo, pues cada árbol de decisión es entrenado en un *dataset* ligeramente diferente y, por ende, tiene un rendimiento ligeramente diferente. A diferencia de los modelos de árboles de decisión normales, como **classification and regression trees (CART)**, los árboles utilizados en el *ensemble* son *unpruned*, lo que los convierte en ligeramente *overfit* para el *dataset*. Esto es deseable, pues ayuda a que cada árbol sea diferente y hayan menos predicciones correlacionadas o errores.\n",
        "\n",
        "Las predicciones de estos árboles son promediadas a través de todos los árboles de decisión, lo que implica un mejor resultado que cualquier árbol individual en el modelo.\n",
        "\n",
        "> **Regression:** prediction is the average prediction across the decision trees.\n",
        "\n",
        "> **Classification:** prediction is the majority vote class label predicted across the decision trees.\n",
        "\n",
        "A diferencia del *bagging*, los *random forest* también involucran seleccionar un subconjunto de las variables de entrada en cada *split point* en la construcción del árbol. Típicamente, construir un árbol de decisión involucra evaluar el valor para cada variable de entrada en los datos para seleccionar un *split point*. Al reducir los *features* a un subconjunto aleatorio que puede ser considerado en cada *split point*, se fuerza que cada árbol de decisión sea distinto.\n",
        "\n",
        "Tal vez el hiperparámetro más importante en el *random forest* es el número de *random features* a considerar en cada *split point*. Una buena heurística para la regresión es asignarle a este hiperparámetro 1/3 de la cantidad de *input features*. En el caso de clasificiación, es adecuado asignarle la raíz cuadrada de la cantidad de *input features*.\n",
        "\n",
        "Otro hiperparámetro importante es la profundidad de los árboles de decisión. A mayor profunidad, mayor probabilidad de *overfitting*, pero también menos correlacionados. Las profunidades entre 1 a 10 niveles pueden ser efectivas. Finalmente, el parámetro de la cantidad de árboles de decisión en el *ensemble* se incrementa hasta que no haya mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWkch5uGlkK"
      },
      "source": [
        "## 2. Random Forest Scikit-Learn API\n",
        "\n",
        "La librería *scikit-learn* provee una implementación de *Random Forest* para *machine learning*.\n",
        "\n",
        "Primero, se confirma que se está usando una versión moderna de la librería:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AI0TxGYh1mn",
        "outputId": "422e3b67-69af-41c2-8500-c628ba42ce30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check scikit-learn version\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.22.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-NUFu7jJbJ"
      },
      "source": [
        "*Random Forest* es provisto via las clases **RandomForestRegressor** y **RandomForestClassifier**. Ambos modelos operan de la misma manera y toman los mismos argumentos que influyen en cómo los árboles de decisión son creados. Durante la construcción del modelo, se utiliza aleatoriedad. Esto significa que, cada vez que se corre el algoritmo, se producirá un modelo ligeramente diferente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phsSaOXAju0q"
      },
      "source": [
        "### Random Forest for Classification\n",
        "\n",
        "Primero, se puede utilizar la función **make_classification()** para crear un *synthetic binary classification problem* con 1000 ejemplos y 20 variables de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ15TC1-yzfy",
        "outputId": "74ec0fe3-5d48-4bfd-c21b-a67e2b4d1d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wU1CkntGpMj"
      },
      "source": [
        "## 3. Random Forest Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoYcvyeGtPm"
      },
      "source": [
        "## 4. Common Questions"
      ]
    }
  ]
}