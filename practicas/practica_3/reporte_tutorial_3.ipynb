{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRTMg-_erfga"
      },
      "source": [
        "# Feature Selection For Machine Learning in Python\n",
        "\n",
        "Los *features* que se utilizan para entrenar los modelos de *machine learning* tienen una influencia enorme en el rendimiento que se puede alcanzar. *Features* irrelevantes o parcialmente irrelevantes pueden afectar negativamente este rendimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj8gRyuls782"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Es el proceso mediante el cual se seleccionan aquellos *features* en los datos que contribuyen en mayor medida a la variable de predicción en la que se está interesado. Los *features* irrelevantes, especialmente en modelos de algoritmos lineales, impactan negativamente los resultados.\n",
        "\n",
        "Tres beneficios de realizar *feature selection* antes de modelar los datos son:\n",
        "\n",
        "- **Reduces Overfitting:** menos datos redundantes significa menos oportunidad para tomar decisiones basadas en ruido.\n",
        "- **Improves Accuracy**\n",
        "- **Reduces Training Time**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHLbDYcumo9"
      },
      "source": [
        "### 1. Univariate Selection\n",
        "\n",
        "Los tests estadísticos se utilizan para seleccionar los *features* que tienen la relación más fuerte con la variable de salida.\n",
        "\n",
        "La librería *scikit-learn* provee la clase **SelectKBest** que puede ser utilizada con una serie de diferentes tests estadísticos para elegir un número específico de *features*.\n",
        "\n",
        "Por ejemplo, el método ANOVA-F es apropiado con variables de entrada numéricas y datos categóricos, mediante la función **f_classif()**. En el siguiente ejemplo, se seleccionan los mejores 4 *features*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CFRL8vwhUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c018b389-47c6-4206-99fd-8a29395ad75f"
      },
      "source": [
        "# Feature Selection with Univariate Statistical Tests\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "# load data\n",
        "filename = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "test = SelectKBest(score_func=f_classif, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "# summarize scores\n",
        "set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "# summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
            "[[  6.  148.   33.6  50. ]\n",
            " [  1.   85.   26.6  31. ]\n",
            " [  8.  183.   23.3  32. ]\n",
            " [  1.   89.   28.1  21. ]\n",
            " [  0.  137.   43.1  33. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmCwtPWT0kAJ"
      },
      "source": [
        "Se pueden ver las puntuaciones de cada atributo y los 4 elegidos (aquellos con puntuaciones mayores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SOaHXcv0qXM"
      },
      "source": [
        "## 2. Recursive Feature Elimination\n",
        "\n",
        "El RFE funciona al eliminar recursivamente atributos y construir un modelo sobre los atributos que se mantengan.\n",
        "\n",
        "El siguiente ejemplo utiliza RFE con la regresión logística para seleccionar los mejores 3 *features*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuFNeQ2U2ANu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a30779-870c-4293-f580-d7771469e060"
      },
      "source": [
        "# Feature Extraction with RFE\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %d\" % fit.n_features_)\n",
        "print(\"Selected Features: %s\" % fit.support_)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoikxUli23OX"
      },
      "source": [
        "## 3. Principal Component Analysis\n",
        "\n",
        "**Principal Component Analysis (PCA)** utiliza álgebra lineal para transformar el *dataset* a una forma comprimida. \n",
        "\n",
        "Por lo general, esta es conocida como una técnica de *data reduction*. Una propiedad del PCA es que se puede elegir el número de dimensiones o componentes principales en el resultado transformado.\n",
        "\n",
        "En el siguiente ejemplo, se utiliza PCA y se seleccionan 3 componentes principales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcDT5dLp22qK",
        "outputId": "1828f5a9-d5df-4316-bed8-767be8f28e82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Feature Extraction with PCA\n",
        "import numpy\n",
        "from pandas import read_csv\n",
        "from sklearn.decomposition import PCA\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "pca = PCA(n_components=3)\n",
        "fit = pca.fit(X)\n",
        "# summarize components\n",
        "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
        "print(fit.components_)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance: [0.88854663 0.06159078 0.02579012]\n",
            "[[-2.02176587e-03  9.78115765e-02  1.60930503e-02  6.07566861e-02\n",
            "   9.93110844e-01  1.40108085e-02  5.37167919e-04 -3.56474430e-03]\n",
            " [-2.26488861e-02 -9.72210040e-01 -1.41909330e-01  5.78614699e-02\n",
            "   9.46266913e-02 -4.69729766e-02 -8.16804621e-04 -1.40168181e-01]\n",
            " [-2.24649003e-02  1.43428710e-01 -9.22467192e-01 -3.07013055e-01\n",
            "   2.09773019e-02 -1.32444542e-01 -6.39983017e-04 -1.25454310e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8XwuwheLYb"
      },
      "source": [
        "Como se puede observar, el *dataset* transformado (con solo los 3 componentes principales) es bastante diferente a los datos originales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ_37uSieSi_"
      },
      "source": [
        "## 4. Feature Importance\n",
        "\n",
        "*Random Forest* y *Extra Trees* pueden ser utilizados para estimar la importancia de los *features*.\n",
        "\n",
        "En el siguiente ejemplo, se construye un *ExtraTrees Classifier* para los *Pima Indians onset* del *dataset* de diabetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIkzRtkxejpC",
        "outputId": "fb81bd43-3450-4489-bc79-19521a2cb4b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Feature Importance with Extra Trees Classifier\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "model = ExtraTreesClassifier(n_estimators=10)\n",
        "model.fit(X, Y)\n",
        "print(model.feature_importances_)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10729378 0.2540643  0.0885615  0.07576402 0.06763607 0.13939648\n",
            " 0.1228383  0.14444556]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUXW6bALelo0"
      },
      "source": [
        "Se puede observar que se le asignó una puntuación de importancia a cada atributo, donde a mayor puntuación mayor importancia. Las puntuaciones sugieren la importancia de *plas, age* y *mass*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SypQ4TJuewyg"
      },
      "source": [
        "## ¿Qué aprendí?\n",
        "\n",
        "Si bien conocía los beneficios de eliminar atributos irrelevantes, no sabía cómo hacerlo, por lo que aprendí 4 nuevas herramientas para trabajar con datos. En particular, me gustó mucho aprender sobre RFE, pues tenía dudas sobre este tema desde tutoriales pasados y creo que, con lo visto en este tutorial, se entiende mejor el concepto.\n",
        "\n",
        "Además, pude observar una aplicación directa de los cursos de matemática que se llevan en la universidad: PCA está basado en álgebra lineal. También, ver cómo herramientas de cursos anteriores (ANOVA, por ejemplo, que fue visto en Diseño de Experimentos) vuelven a aparecer en el trabajo con datos, esta vez con aplicaciones distintas.\n",
        "\n",
        "Finalmente, aprendí cómo los *Random Forest* tienen su aplicación también en el *feature selection*. Este tema me llama mucho la atención y me gustó tener un primer acercamiento con su utilidad.\n",
        "\n",
        "## ¿Qué me genera dudas?\n",
        "\n",
        "1. ¿Cómo saber cuál (o cuáles) métodos elegir? Basado en tutoriales anteriores, supongo que depende de los datos a trabajar y la experiencia que se tenga, pero tengo curiosidad si hay una \"regla general\" que pueda ayudar a decidir.\n",
        "2. Me interesa conocer la base matemática de estos métodos, en particular el de PCA. ¿Cuál sería un buen recurso (libro, etc) que permita ahondar en la parte matemática más formal de estos contenidos?\n",
        "3. ¿*Data reduction* es lo mismo que *dimensionality reduction*?"
      ]
    }
  ]
}