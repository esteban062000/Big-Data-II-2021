{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRTMg-_erfga"
      },
      "source": [
        "# Feature Selection For Machine Learning in Python\n",
        "\n",
        "Los *features* que se utilizan para entrenar los modelos de *machine learning* tienen una influencia enorme en el rendimiento que se puede alcanzar. *Features* irrelevantes o parcialmente irrelevantes pueden afectar negativamente este rendimiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj8gRyuls782"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Es el proceso mediante el cual se seleccionan aquellos *features* en los datos que contribuyen en mayor medida a la variable de predicción en la que se está interesado. Los *features* irrelevantes, especialmente en modelos de algoritmos lineales, impactan negativamente los resultados.\n",
        "\n",
        "Tres beneficios de realizar *feature selection* antes de modelar los datos son:\n",
        "\n",
        "- **Reduces Overfitting:** menos datos redundantes significa menos oportunidad para tomar decisiones basadas en ruido.\n",
        "- **Improves Accuracy**\n",
        "- **Reduces Training Time**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHLbDYcumo9"
      },
      "source": [
        "### 1. Univariate Selection\n",
        "\n",
        "Los tests estadísticos se utilizan para seleccionar los *features* que tienen la relación más fuerte con la variable de salida.\n",
        "\n",
        "La librería *scikit-learn* provee la clase **SelectKBest** que puede ser utilizada con una serie de diferentes tests estadísticos para elegir un número específico de *features*.\n",
        "\n",
        "Por ejemplo, el método ANOVA-F es apropiado con variables de entrada numéricas y datos categóricos, mediante la función **f_classif()**. En el siguiente ejemplo, se seleccionan los mejores 4 *features*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CFRL8vwhUG",
        "outputId": "592a8f17-6a0c-4a5b-842a-dce667178fba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Feature Selection with Univariate Statistical Tests\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "# load data\n",
        "filename = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "test = SelectKBest(score_func=f_classif, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "# summarize scores\n",
        "set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "# summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 39.67  213.162   3.257   4.304  13.281  71.772  23.871  46.141]\n",
            "[[  6.  148.   33.6  50. ]\n",
            " [  1.   85.   26.6  31. ]\n",
            " [  8.  183.   23.3  32. ]\n",
            " [  1.   89.   28.1  21. ]\n",
            " [  0.  137.   43.1  33. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmCwtPWT0kAJ"
      },
      "source": [
        "Se pueden ver las puntuaciones de cada atributo y los 4 elegidos (aquellos con puntuaciones mayores)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SOaHXcv0qXM"
      },
      "source": [
        "## 2. Recursive Feature Elimination\n",
        "\n",
        "El RFE funciona al eliminar recursivamente atributos y construir un modelo sobre los atributos que se mantengan.\n",
        "\n",
        "El siguiente ejemplo utiliza RFE con la regresión logística para seleccionar los mejores 3 *features*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuFNeQ2U2ANu",
        "outputId": "5d198471-40dc-4c55-f621-a79086dd4c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Feature Extraction with RFE\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# load data\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "# feature extraction\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %d\" % fit.n_features_)\n",
        "print(\"Selected Features: %s\" % fit.support_)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 4 5 6 1 1 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoikxUli23OX"
      },
      "source": [
        "## 3. Principal Component Analysis\n",
        "\n",
        "**Principal Component Analysis (PCA)** utiliza álgebra lineal para transformar el *dataset* a una forma comprimida. \n",
        "\n",
        "Por lo general, esta es conocida como una técnica de *data reduction*. Una propiedad del PCA es que se puede elegir el número de dimensiones o componentes principales en el resultado transformado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcDT5dLp22qK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}