{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3woOvidFYCR"
      },
      "source": [
        "# How to Choose a Feature Selection Method For Machine Learning\n",
        "\n",
        "> ***Feature selection:*** proceso para reducir el número de variables de entrada al desarrollar un modelo predictivo.\n",
        "\n",
        "Al reducir la cantidad de variables de entrada, se reduce también el **costo computacional** del modelado y, algunas veces, **mejora** el rendimiento del modelo.\n",
        "\n",
        "Los métodos basados en estadística evaluán la relación entre cada variable de entrada y la variable de salida mediante estadística, para luego seleccionar aquellas que tienen relaciones más fuertes. Estos métodos pueden ser rápidos y efectivos, pero la elección de cuáles medidas estadísticas utilizar depende del tipo de datos de entrada y salida.\n",
        "\n",
        "## 1. Feature Selection Methods\n",
        "\n",
        "> ***Feature selection*** se centra, principalmente, en remover variables redundantes o poco informativas.\n",
        "\n",
        "Una manera de pensar en los métodos de *feature selection* es en términos de **supervisados** (utilizan la variable de salida) y **no supervisados** (ignoran la variable de salida; por ejemplo, correlación). Otra forma de pensar en estos métodos es que se pueden dividir en **wrapper** (agrupadores) y **filter** (filtradores). Estos métodos casi siempre son supervisados y se evalúan según el rendimiento del modelo.\n",
        "\n",
        "Los métodos *wrapper* crean muchos modelos con diferentes subconjuntos de las variables de entrada y seleccionan aquellos que produzcan mejores resultados predictivos. Pueden ser computacionalmente costosos, pero no se preocupan por el tipo de las variables. Un ejemplo es **Recursive feature elimination (RFE)**.\n",
        "\n",
        "Los métodos *filter* utilizan técnicas estadísticas que evalúan la relación entre cada variable de entrada y la variable de salida, para luego utilizar estos valores como la base para elegir (filtrar) cuáles variables de entrada serán utilizadas en el modelo.\n",
        "\n",
        "Finalmente, existen algunos modelos de *machine learning* que realizan *feature selection* automáticamente como parte del proceso de aprendizaje del modelo. Estas técnicas se conocen como **intrinsic** *feature selection methods*. En esta categoría se incluyen algoritmos como *penalized regression models* como Lasso y árboles de decisión.\n",
        "\n",
        "La diferencia entre *feature selection* y *dimensionality reduction* es que, el primero, selecciona *features* para mantener o remover del *dataset*, mientras que el segundo crea una **proyección** de los datos, lo que resulta en variables de entrada completamente nuevas. Por ende, *dimensionality reduction* es una alternativa a *feature selection*, **no** un tipo de *feature selection*.\n",
        "\n",
        "Los distintos métodos de **feature selection** se pueden organizar de la siguiente manera:\n",
        "\n",
        "*   **No supervisados:** no utilizan la variable de salida (remueven variables redundantes).\n",
        "  * Correlación.\n",
        "* **Supervisados:** utiliza la variable de salida (remueven variables irrelevantes).\n",
        "  * **Wrapper:** busca subconjuntos de *features* que tengan buenos resultados.\n",
        "    * RFE.\n",
        "  * **Filter:** selecciona subconjuntos de *features* basados en su relación con la variable de salida.\n",
        "    * Métodos estadísticos.\n",
        "    * Métodos de *feature importance*.\n",
        "  * **Intrinsic:** algoritmos que realizan la detección de *features* automáticamente durante el proceso de aprendizaje.\n",
        "    * Árboles de decisión.\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2019/11/Overview-of-Feature-Selection-Techniques3.png)\n",
        "\n",
        "## 2. Statistics for Filter-Based Feature Selection Methods\n",
        "\n",
        "Es común utilizar medidas estadísticas como la correlación entre variables de entrada y salida como la base para filtrar *features*.\n",
        "\n",
        "La siguiente imagen resume los principales tipos de datos:\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2020/06/Overview-of-Data-Variable-Types2.png)\n",
        "\n",
        "Mientras más conocimiento se tenga sobre alguna variable, más fácil será elegir una medida estadística apropiada para un método basado en filtrado.\n",
        "\n",
        "En esta sección, se consideran dos grupos amplios de variables: *numéricas* y *categóricas*.\n",
        "\n",
        "> **Numerical output:** regression predictive modeling problem.\n",
        "> **Categorical output:** classification predictive modeling problem.\n",
        "\n",
        "Las medidas estadísticas utilizadas en los métodos basados en filtrado, por lo general, se calculan con una variable de entrada cada vez. Por ende, se conocen como **univariate statistical measures**, esto puede significar que las interacciones entre las variables de entrada no son consideradas en el proceso de filtrado.\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)\n",
        "\n",
        "### Numerical Input, Numerical Output\n",
        "\n",
        "Las técnicas más comunes consisten en utilizar un coeficiente de correlación, como el de Pearson para una correlación linear o métodos basados en ranking para relaciones no lineares.\n",
        "\n",
        "- Pearson´s correlation coefficient (linear).\n",
        "- Spearman´s rank coefficient (nonlinear).\n",
        "\n",
        "### Numerical Input, Categorical Output\n",
        "\n",
        "Este puede ser el ejemplo más común de un problema de clasificación. Nuevamente, las técnicas más populares están basadas en correlación, aunque deben tomar en cuenta los datos categóricos.\n",
        "\n",
        "- ANOVA correlation coefficient (linear).\n",
        "- Kendall´s rank coefficient (nonlinear).\n",
        "\n",
        "> Kendall asume que la variable categórica es ordinal.\n",
        "\n",
        "### Categorical Input, Numerical Output\n",
        "\n",
        "Este es un ejemplo extraño de un problema de regresión. Sin embargo, se puede usar el mismo enfoque de *\"Numerical Input, Categorical Output* descrito anteriormente, pero en reversa.\n",
        "\n",
        "### Categorical Input, Categorical Output\n",
        "\n",
        "La medida de correlación más común para datos categóricos es el **chi-squared test**. También se puede usar **mutual information (information gain)** del campo de la teoría de la información.\n",
        "\n",
        "- Chi-squared test (contingency tables).\n",
        "- Mutual information.\n",
        "\n",
        "El método de *mutual information* es independiente del tipo de datos.\n",
        "\n",
        "## 3. Tips and Tricks for Feature Selection\n",
        "\n",
        "### Correlation Statistics\n",
        "\n",
        "La librería *scikit-learn* provee una implementación de las medidas estadísticas más útiles, por ejemplo:\n",
        "\n",
        "- Pearson’s Correlation Coefficient: *f_regression()*\n",
        "- ANOVA: *f_classif()*\n",
        "- Chi-Squared: *chi2()*\n",
        "- Mutual Information: *mutual_info_classif()* and *mutual_info_regression()*\n",
        "\n",
        "También, la librería *SciPy* provee implementaciones de muchas más medidas, como *Kendall´s tau* y *Spearman´s rank correlation*.\n",
        "\n",
        "### Selection Method\n",
        "\n",
        "La librería *scikit-learn* también provee varios métodos de filtrado una vez se han calculado los estadísticos para cada variable de entrada con respecto a la salida. Dos de los más populares son:\n",
        "\n",
        "- Seleccionar las mejores *k* variables: **SelectKBest**.\n",
        "- Seleccionar el mejor percentil de variables: **SelectPercentile**.\n",
        "\n",
        "### Transform Variables\n",
        "\n",
        "Considere transformar variables para acceder a distintos métodos estadísticos. Por ejemplo, transformar una variable categórica a una ordinal y revisar qué resultados interesantes produce. También, se pueden discretizar variables numéricas.\n",
        "\n",
        "Algunas medidas estadísticas asumen propiedades de las variables, como el coeficiente de Pearson que asume una distribución normal de los datos. Se pueden transformar los datos para cumplir con estas expectativas y revisar los resultados.\n",
        "\n",
        "### What is the Best Method?\n",
        "\n",
        "**No hay un método mejor para selección de *features***. Se debe descubrir cuál es el más adecuado para cada problema en específico.\n",
        "\n",
        "## 4. Worked Examples of Feature Selection\n",
        "\n"
      ]
    }
  ]
}