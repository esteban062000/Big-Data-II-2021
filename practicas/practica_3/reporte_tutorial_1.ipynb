{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lab2_Notes.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3woOvidFYCR"
      },
      "source": [
        "# How to Choose a Feature Selection Method For Machine Learning\n",
        "\n",
        "> ***Feature selection:*** proceso para reducir el número de variables de entrada al desarrollar un modelo predictivo.\n",
        "\n",
        "Al reducir la cantidad de variables de entrada, se reduce también el **costo computacional** del modelado y, algunas veces, **mejora** el rendimiento del modelo.\n",
        "\n",
        "Los métodos basados en estadística evaluán la relación entre cada variable de entrada y la variable de salida mediante estadística, para luego seleccionar aquellas que tienen relaciones más fuertes. Estos métodos pueden ser rápidos y efectivos, pero la elección de cuáles medidas estadísticas utilizar depende del tipo de datos de entrada y salida.\n",
        "\n",
        "## 1. Feature Selection Methods\n",
        "\n",
        "> ***Feature selection*** se centra, principalmente, en remover variables redundantes o poco informativas.\n",
        "\n",
        "Una manera de pensar en los métodos de *feature selection* es en términos de **supervisados** (utilizan la variable de salida) y **no supervisados** (ignoran la variable de salida; por ejemplo, correlación). Otra forma de pensar en estos métodos es que se pueden dividir en **wrapper** (agrupadores) y **filter** (filtradores). Estos métodos casi siempre son supervisados y se evalúan según el rendimiento del modelo.\n",
        "\n",
        "Los métodos *wrapper* crean muchos modelos con diferentes subconjuntos de las variables de entrada y seleccionan aquellos que produzcan mejores resultados predictivos. Pueden ser computacionalmente costosos, pero no se preocupan por el tipo de las variables. Un ejemplo es **Recursive feature elimination (RFE)**.\n",
        "\n",
        "Los métodos *filter* utilizan técnicas estadísticas que evalúan la relación entre cada variable de entrada y la variable de salida, para luego utilizar estos valores como la base para elegir (filtrar) cuáles variables de entrada serán utilizadas en el modelo.\n",
        "\n",
        "Finalmente, existen algunos modelos de *machine learning* que realizan *feature selection* automáticamente como parte del proceso de aprendizaje del modelo. Estas técnicas se conocen como **intrinsic** *feature selection methods*. En esta categoría se incluyen algoritmos como *penalized regression models* como Lasso y árboles de decisión.\n",
        "\n",
        "La diferencia entre *feature selection* y *dimensionality reduction* es que, el primero, selecciona *features* para mantener o remover del *dataset*, mientras que el segundo crea una **proyección** de los datos, lo que resulta en variables de entrada completamente nuevas. Por ende, *dimensionality reduction* es una alternativa a *feature selection*, **no** un tipo de *feature selection*.\n",
        "\n",
        "Los distintos métodos de **feature selection** se pueden organizar de la siguiente manera:\n",
        "\n",
        "*   **No supervisados:** no utilizan la variable de salida (remueven variables redundantes).\n",
        "  * Correlación.\n",
        "* **Supervisados:** utiliza la variable de salida (remueven variables irrelevantes).\n",
        "  * **Wrapper:** busca subconjuntos de *features* que tengan buenos resultados.\n",
        "    * RFE.\n",
        "  * **Filter:** selecciona subconjuntos de *features* basados en su relación con la variable de salida.\n",
        "    * Métodos estadísticos.\n",
        "    * Métodos de *feature importance*.\n",
        "  * **Intrinsic:** algoritmos que realizan la detección de *features* automáticamente durante el proceso de aprendizaje.\n",
        "    * Árboles de decisión.\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2019/11/Overview-of-Feature-Selection-Techniques3.png)\n",
        "\n",
        "## 2. Statistics for Filter-Based Feature Selection Methods\n",
        "\n",
        "Es común utilizar medidas estadísticas como la correlación entre variables de entrada y salida como la base para filtrar *features*.\n",
        "\n",
        "La siguiente imagen resume los principales tipos de datos:\n",
        "\n",
        "![image.png](https://machinelearningmastery.com/wp-content/uploads/2020/06/Overview-of-Data-Variable-Types2.png)\n",
        "\n",
        "Mientras más conocimiento se tenga sobre alguna variable, más fácil será elegir una medida estadística apropiada para un método basado en filtrado.\n"
      ]
    }
  ]
}