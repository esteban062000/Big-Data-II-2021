En las primeras semanas del curso, se ha tomado un enfoque introductorio a los diferentes aspectos que existen en el análisis de datos. Por ejemplo, se han ofrecido definiciones concisas sobre conceptos como *data mining*, *big data* y *data science*, además de una breve reseña sobre distintas herramientas que se utilizan para trabajar en estas áreas. De manera muy general, se ha ofrecido un vistazo sobre la *big picture* del curso, para entender mejor dónde estamos, hacia dónde vamos y por qué es necesaria el área.

También, se ha profundizado en temáticas como la recolección de datos y la limpieza de los mismos. Con el primer tema, se han mencionado alternativas ingeniosas (por ejemplo, *crowdsourcing*) cuando se tienen dificultades para recolectar los datos, por ejemplo, cuando las fuentes de información se encuentran en un formato digitalmente no disponible, como papel o gráficos en pdf que no se pueden consultar.

Con respecto a la limpieza de datos, se ha enfatizado en el **por qué** es importante, además de mencionar los posibles errores en los que se puede incurrir si no se cumple (unidades diferentes en los datos, valores con "código", valores ambiguos, etc). En clase se mencionó que, aproximadamente, del 10% al 60% del tiempo del trabajo en el análisis de datos se invierte en la limpieza y preparación de los datos, por lo que definitivamente es una sección de extrema importancia en todo el proceso. Como también se mencionó, se debe evitar el error de *garbage in, gargabe out*.

Personalmente, me llamó mucho la atención la importancia otorgada a la limpieza de datos. En el curso anterior de Diseño de Experimentos, muchas veces, se asumía que los datos vienen limpios y listos para ser procesados, cuando esto no es así en el mundo real. Creo que, con el enfoque llevado hasta ahora, se le ha dado prioridad a desarrollar habilidades necesarias para trabajar con los conjuntos de datos que, posiblemente, se encuentran al realizar investigaciones más formales. En particular, tengo la impresión de que no hay un *overlap* preocupante entre ambos cursos, sino que son más bien un complemento: en Diseño de Experimentos, se aprendió a trabajar sobre lo que está listo; en Grandes Volúmenes, se utilizan las mismas herramientas que se aprendieron en Diseño, pero aplicadas a un contexto más real y, naturalmente, más retador.

Hasta el momento, no he tenido dudas sobre la materia expuesta, más allá de la curiosidad inicial de hasta dónde puede llegar la *Big Data*, tanto en términos de volumen de datos como en capacidad de procesamiento. En lo personal, me gustaría desempeñarme en el futuro como *data scientist*, por lo que he encontrado de utilidad el material estudiado. De manera particular, me siento más cómodo profesionalmente al enfrentar desafíos reales (como conjuntos de datos sin limpiar), aunado al conocimiento ya adquirido en cursos anteriores. Aunado a lo anterior, las ideas sobre cuestiones legales (por ejemplo, el uso de información confidencial o violar los términos de servicio al realizar *web scraping*) contribuyen a la formación integral y, en el mejor de los casos, pueden ayudar a evitar un conflicto.

Con respecto a material extra empleado, anteriormente había consultado el siguiente [material de Coursera](https://www.coursera.org/specializations/big-data), que ofrece también información interesante sobre el análisis de grandes volúmenes de datos.

