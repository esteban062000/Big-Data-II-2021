En las últimas dos semanas del curso, se concluyó con el tema de limpieza y preparación de datos, además de iniciar con el tema de modelos y cómo se construyen. Creo que es una buena transición en la temática del curso, pues se le dedicó suficiente tiempo a la parte fundamental de la limpieza de datos, para ahora poder utilizar esos mismos datos, ya limpios, para obtener información de utilidad.

Con la práctica 2, se mostraron técnicas para eliminar valores atípicos (*ouliers*), identificar columnas con pocos valores únicos o poca varianza. Estos temas en particular fueron de mucho provecho para mí, pues la única técnica que conocía era la del rango intercuartiles (desconocía el método de la desviación estándar y la detección automática de *outliers*, por ejemplo). También, se mencionaron datos téoricos importantes, como que el 68% de los datos en una distribución normal se encuentran a menos de una desviación estándar de la media. 

El curso ha funcionado como repaso de la teoría aprendida en cursos anteriores, como Probabilidad y Estadística y Diseño de Experimentos. Creo que, ahora, se pueden aprovechar mejor estos repasos, pues permiten recordar datos que se habían olvidado o profundizar en temas que, gracias a la madurez del tiempo, se han entendido mejor a lo largo de los cursos.

También, en la tarea 1, se utilizó la herramienta OpenRefine para limpiar un conjunto de datos. Esta tarea me gustó y creo que fue de mucha utilidad, pues ha sido la primera vez que trabajo con un dataset "real" y no uno ya limpio de Kaggle. El proceso de limpieza fue retador al inicio, pero ejemplificó en la práctica la importancia de preparar los datos antes de analizarlos, con el fin de facilitar la tarea de obtener conclusiones significativas.

Sobre la misma línea de pre-procesamiento de datos, se abordó el tema de transformación de atributos, que también era desconocido para mí. En él, se mostraron distintos enfoques para reducir la dimensionalidad del dataset, obtener información nueva a partir de atributos o "formatear" los datos mediante técnicas como estandarización o normalización. Puesto que conocía muy poco sobre estas técnicas, esa charla en particular fue de mucho provecho para mí.

Finalmente, hacia las últimas clases, se inició con el tema de algoritmos y modelos. En la primera clase introductoria, se hizo un repaso de materia relacionada con Inteligencia Artificial y Machine Learning, que había sido tratada previamente en otros cursos. Sin embargo, me parece interesante ver cómo esos mismos fundamentos se pueden aplicar a un campo distinto como lo es Big Data.

Hasta el momento, no tengo dudas sobre la materia cubierta. Con los modelos y algoritmos para trabajar con los datos, espero poder aprender más sobre estas áreas y aplicarlas junto con los conocimientos que he adquirido anteriormente. En particular, aunado con las diversas herramientas que he adquirido en el curso, me gustaría aplicarlas para un estudio que, hasta el momento, no había podido realizar por el desconocimiento técnico: procesar la base de datos de partidas de Lichess.com (con varios millones de partidas cada mes), para concluir si la serie de Netflix "The Queen´s Gambit" produjo un aumento en la cantidad de partidas que empezaron con la apertura del Gambito de Dama. Así como pienso aplicarlo en este proyecto, podría aplicarlo en otra área según sea necesario, pues creo que el curso ha provisto los fundamentos necesarios para adaptarse a las distintas circunstancias que puedan surgir. En estas dos semanas, no he utilizado material extra en el aprendizaje.